{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Entity Resolution with Splink using Spark",
   "id": "bd551a95d6220003"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from splink.spark.jar_location import similarity_jar_location\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import warnings\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.default.parallelism\", \"50\")\n",
    "conf.set(\"spark.sql.shuffle.partitions\", \"50\")\n",
    "conf.set(\"spark.executor.memory\", \"6g\")  # Set to 6 gigabytes\n",
    "conf.set(\"spark.driver.memory\", \"6g\")  # Set to 6 gigabytes\n",
    "\n",
    "# Add custom similarity functions, which are bundled with Splink\n",
    "# documented here: https://github.com/moj-analytical-services/splink_scalaudfs\n",
    "path = similarity_jar_location()\n",
    "conf.set(\"spark.jars\", path)\n",
    "\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "\n",
    "spark = SparkSession(sc)\n",
    "spark.sparkContext.setCheckpointDir(\"./tmp_checkpoints\")\n",
    "\n",
    "# Disable warnings for pyspark / hadoop\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "warnings.simplefilter(\"ignore\", UserWarning)"
   ],
   "id": "4cb93eb2776c8e88",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Read in the data and view a sample\n",
    "df = spark.read.csv('/Users/chase.burkhalter/Documents/DBeaver/Exports/TEMP_SPLINK_DATA_202404291432_ALL.csv', header=True, inferSchema=True)\n",
    "\n",
    "df.show(10)"
   ],
   "id": "ddf140021e2feb2e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T16:36:17.536946Z",
     "start_time": "2024-04-30T16:36:17.500189Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from splink.spark.linker import SparkLinker\n",
    "#from splink.spark.blocking_rule_library import block_on\n",
    "\n",
    "# Define the settings for the linker\n",
    "settings = {\n",
    "    \"link_type\": \"dedupe_only\",\n",
    "    \"blocking_rules_to_generate_predictions\": [\n",
    "        \"l.anonymous_id = r.anonymous_id\",\n",
    "        \"l.user_id = r.user_id\",\n",
    "        \"l.email = r.email\"\n",
    "    ],\n",
    "    \"retain_matching_columns\": False,\n",
    "    \"retain_intermediate_calculation_columns\": False\n",
    "}\n",
    "\n",
    "# Instantiate the linker with Spark\n",
    "linker = SparkLinker(df, settings, spark=spark)"
   ],
   "id": "5267d3f384cc4677",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Estimate the probability that two random records match\n",
    "for rule in settings[\"blocking_rules_to_generate_predictions\"]:\n",
    "    prob = linker.estimate_probability_two_random_records_match(rule, recall=0.9)\n",
    "    print(f\"Probability two random records match for '{rule}': {prob}\")\n",
    "    \n",
    "# Count the number of comparisons generated by blocking rules\n",
    "for rule in settings[\"blocking_rules_to_generate_predictions\"]:\n",
    "    count = linker.count_num_comparisons_from_blocking_rule(rule)\n",
    "    print(f\"Number of comparisons generated by '{rule}': {count}\")"
   ],
   "id": "2d7b5755fe78b0b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Use deterministic rules link\n",
    "df_deterministic = linker.deterministic_link()\n",
    "df_deterministic.as_pandas_dataframe(10)"
   ],
   "id": "fea29eed103df0b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cluster the deterministic links based on threshold\n",
    "clusters_deterministic = linker.cluster_pairwise_predictions_at_threshold(df_deterministic, threshold_match_probability=0.9)\n",
    "clusters_deterministic.as_pandas_dataframe()"
   ],
   "id": "89d802c4b79446b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initiate the cluster studio dashboard to view the clusters \n",
    "linker.cluster_studio_dashboard(df_deterministic, clusters_deterministic, \"cluster_studio.html\", sampling_method=\"by_cluster_size\", overwrite=True)\n",
    "from IPython.display import IFrame\n",
    "IFrame(\n",
    "    src=\"cluster_studio.html\", width=\"100%\", height=1100\n",
    ")"
   ],
   "id": "c0abe49fc2f4cfb9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "identityresolution",
   "language": "python",
   "name": "identityresolution"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
